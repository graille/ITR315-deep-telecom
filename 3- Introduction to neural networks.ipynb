{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single layer perceptron\n",
    "\n",
    "##   Principle\n",
    "\n",
    "A perceptron is a building block of artifical neural networks described as follows: \n",
    "\n",
    "<img src=\"figs/ArtificialNeuron.png\" width=\"250\"> \n",
    "\n",
    "- $x^n = (x_1, ..., x_n)$ is the input array \n",
    "- $v_j$ is the produced output \n",
    "- $(w_{j,1}, w_{j,n})$ are the weights associated to each of the synapses\n",
    "- The activation function is a threshold function\n",
    "- $b_j$ is the firing threshold \n",
    "\n",
    "The perpectron output is given by the equation \n",
    "\n",
    "$$ v_j = \\mathbb{1} \\left( \\sum_{i=1}^n w_{j,i} x_i > b_j \\right) $$\n",
    "\n",
    "N.B. Often, by rewriting the output of the perceptron as follows \n",
    "\n",
    "$$ v_j = \\mathbb{1} \\left( \\sum_{i=1}^n w_{j,i} . x_i - b_j.1 > 0 \\right) $$\n",
    "\n",
    "we can assume that the biais is a trainable weight itself associated with an extra input whose value is 1. Thus, the input now becomes $x^n = (x_1, ..., x_n , 1)$ and the weights are given by $(w_{j,1}, w_{j,n}, -b_j)$. This node is called a bias node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Function approximation: linear separation\n",
    "\n",
    "A perceptron allows to perform a linear classification task over linearly separable spaces as shown in the figure. \n",
    "\n",
    "<img src=\"figs/ArtificialNeuron.png\" width=\"250\"> \n",
    "\n",
    "\n",
    "In the following, we give a few simple examples to linear classification over GF(2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the \"OR\" and \"AND\"\n",
    "\n",
    "Assume that the input space has dimension 2 and consists of binary pairs $(x_1, x_2) \\in GF(2)^2$. In the following, we want to approximate the function OR of both bits $(x_1, x_2)$. \n",
    "\n",
    "- 1. Write the binary table of the OR and AND operations\n",
    "- 2. Find two weights $(w_1,w_2)$ and a biais $b$ such that the result of the perceptron is the OR, i.e., \n",
    "$$\\mathbb{1} \\left(   w_{1}. x_1 + w_2. x_2  > b  \\right) = x_1 \\hat{} x_2 $$\n",
    "- 3. Do the same for the AND operation\n",
    "\n",
    "Hint: think graphical :) ! "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the \"XOR\"\n",
    "\n",
    "Assume that the input space has dimension 2 and consists of binary pairs $(x_1, x_2) \\in GF(2)^2$ and let us build a perceptron to match the XOR of both bits $(x_1, x_2)$. \n",
    "\n",
    "- 1. Write the binary table of the XOR operation\n",
    "- 2. Find two weights $(w_1,w_2)$ and a biais $b$ such that the result of the perceptron is the OR, i.e., \n",
    "$$\\mathbb{1} \\left(   w_{1}. x_1 + w_2. x_2  > b  \\right) = x_1  \\oplus x_2 $$\n",
    "- 3. Is the XOR linearly separable? Suggest a solution ... \n",
    "\n",
    "Hint: again, think graphical :) ! \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron (MLP)\n",
    "\n",
    "A Multi-layer perceptron is the starting idea of neural networks, and it consists of stacking two layers, or more, of perceptron. The  MLP consists thus in an input layer, hidden layers, and an output layer. \n",
    "\n",
    "Question: Assume that the input space has dimension n and consists in a binary stream $(x_1, x_2, ..., x_n) \\in GF(2)^n$ and let us build an MLP to compute the parity (XOR) of all bits $(x_1 \\oplus  x_2  \\oplus , ...,  \\oplus x_n)$. \n",
    " \n",
    "Hint: again, two layers are sufficient! "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning basics\n",
    "\n",
    "## ML paradigms\n",
    "\n",
    "There are three families of machine learning paradigms:\n",
    "- Supervised learning: neural networks, support vector machines (SVM), linear regression, naive Bayes, ... \n",
    "- Unsupervised learning: auto-encoders, clustering, generative adverserial networks (GAN)  \n",
    "- Reinforcement learning: games \n",
    "\n",
    "In this course, we focus on supervised learning, and more specifically, on neural networks. If you are curious, a nice introduction is given in D. Kriesel's book, available online. (http://www.dkriesel.com/en/science/neural_networks) \n",
    "\n",
    "\n",
    "##  Supervised learning: Neural networks\n",
    "\n",
    "The main components in a supervised learning problem based on a neural networs are: the dataset, the loss function, and the network's structure. \n",
    "<img src=\"figs/Meteparameters.png\" width=\"400\">\n",
    "\n",
    "- The dataset: the set of pairs $(y^n, u^k)$ with $y^n$ the inputs of the network, and $u^k$ its outputs. \n",
    "- The loss function $L$: most commonly used are mean squared error (MSE) and binary crossentropy\n",
    "- The structure of the network: \n",
    "    - Backbone: feedforward, Recursive Neural Net (RNN), Restricted Boltzman Machine (RBM), Convolutional NN (CNN), ...\n",
    "    - Activation functions: sigmoid, ReLu, leaky Relu, tanh , ... \n",
    "    - Depth/ width, array/ one hot, ... \n",
    "    - Weights W, biases B, ... \n",
    " \n",
    "Training a neural network amounts to solving the problem \n",
    "\n",
    "$$ (W^\\star, B^\\star) = \\underset{W,B}{argmin } \\  \\mathbb{E}_{(y^n, u^k)} \\left( L\\left(U^k,\\hat{U}^k\\right) \\right) $$\n",
    "\n",
    "where the $\\hat{U}^k$ are the outputs of the neural network, and the expectation $\\mathbb{E}_{(y^n, u^k)} (\\cdot)$ is approximated with an empirical mean over the data set. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
